{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pendle_llm = pd.read_csv(\"pendle_llm_analysis.csv\")\n",
    "tweet_df = pd.read_json(\"pendle.json\")\n",
    "\n",
    "author_df = tweet_df[\"author\"].apply(pd.Series)\n",
    "unique_authors_df = author_df.drop_duplicates(subset=['userName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendle_llm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_authors_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = pendle_llm.isna().sum()\n",
    "missing_summary = missing_values[missing_values > 0].to_dict()\n",
    "\n",
    "if missing_summary:\n",
    "    print(missing_summary)\n",
    "else:\n",
    "    print(\"No missing values found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_authors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_analysis_merged = pd.merge(tweet_df,pendle_llm,on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_analysis_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_analysis_merged.to_csv(\"merged_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_authors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_username(author_obj):\n",
    "    try:\n",
    "        return author_obj.get(\"userName\", None)  # Extract userName\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None  # Return None if parsing fails\n",
    "\n",
    "\n",
    "tweet_analysis_merged[\"userName\"] = tweet_analysis_merged[\"author\"].apply(extract_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_analysis_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_analysis_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Vs Emotional Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_author_ratios(df):\n",
    "    # Group by userName and count tweet types\n",
    "    tweet_counts = df.groupby(\"userName\")[\"tweet_type\"].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    tweet_counts[\"total_tweets\"] = tweet_counts.sum(axis=1)\n",
    "\n",
    "    tweet_counts[\"stat_vs_emot_ratio\"] = tweet_counts[\"statistical\"] / tweet_counts[\"total_tweets\"]\n",
    "\n",
    "    return tweet_counts.reset_index()\n",
    "\n",
    "author_stat_ratios_df = compute_author_ratios(tweet_analysis_merged)\n",
    "\n",
    "print(author_stat_ratios_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_stat_ratios_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historical Comparison Presence Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_author_ratios(df):\n",
    "    # Group by userName and count tweet types\n",
    "    tweet_counts = df.groupby(\"userName\")[\"historical_comparison\"].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    tweet_counts[\"total_tweets\"] = tweet_counts.sum(axis=1)\n",
    "\n",
    "    tweet_counts[\"historical_comparison_ratio\"] = tweet_counts[\"present\"] / tweet_counts[\"total_tweets\"]\n",
    "\n",
    "    return tweet_counts.reset_index()\n",
    "\n",
    "author_hist_ratios_df = compute_author_ratios(tweet_analysis_merged)\n",
    "\n",
    "author_hist_ratios_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Market Hint Classification Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_author_ratios(df):\n",
    "    # Group by userName and count tweet types\n",
    "    tweet_counts = df.groupby(\"userName\")[\"market_hint\"].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    tweet_counts[\"total_tweets\"] = tweet_counts.sum(axis=1)\n",
    "\n",
    "    tweet_counts[\"market_hint_ratio\"] = tweet_counts[\"signal\"] / tweet_counts[\"total_tweets\"]\n",
    "\n",
    "    return tweet_counts.reset_index()\n",
    "\n",
    "author_hint_ratios_df = compute_author_ratios(tweet_analysis_merged)\n",
    "\n",
    "author_hint_ratios_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical columns\n",
    "category_columns = [\"signal_classification\", \"call_to_action\", \"hype_classification\", \"urgency_level\"]\n",
    "\n",
    "# Optimized approach: Compute category ratios separately and merge results\n",
    "category_ratios_list = []\n",
    "\n",
    "for col in category_columns:\n",
    "    temp_df = tweet_analysis_merged.groupby(\"userName\")[col].value_counts(normalize=True).unstack(fill_value=0)\n",
    "    temp_df.columns = [f\"{col}_{val}_ratio\" for val in temp_df.columns]\n",
    "    category_ratios_list.append(temp_df)\n",
    "\n",
    "# Merge all computed ratios\n",
    "final_ratios = pd.concat(category_ratios_list, axis=1).reset_index()\n",
    "\n",
    "# Display the first few rows\n",
    "final_ratios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weighted scores for each category separately\n",
    "final_ratios[\"signal_classification_score\"] = (\n",
    "    final_ratios[\"signal_classification_bearish_ratio\"] * 0 +\n",
    "    final_ratios[\"signal_classification_bullish_ratio\"] * 1 +\n",
    "    final_ratios[\"signal_classification_normal_ratio\"] * 0.5\n",
    ")\n",
    "\n",
    "final_ratios[\"call_to_action_score\"] = (\n",
    "    final_ratios[\"call_to_action_buy_ratio\"] * 1 +\n",
    "    final_ratios[\"call_to_action_sell_ratio\"] * 0.2 +\n",
    "    final_ratios[\"call_to_action_none_ratio\"] * 0 +\n",
    "    final_ratios[\"call_to_action_hold_ratio\"] * 0.5\n",
    ")\n",
    "\n",
    "final_ratios[\"hype_classification_score\"] = (\n",
    "    final_ratios[\"hype_classification_high_ratio\"] * 0 +\n",
    "    final_ratios[\"hype_classification_normal_ratio\"] * 0.5 +\n",
    "    final_ratios[\"hype_classification_low_ratio\"] * 1\n",
    ")\n",
    "\n",
    "final_ratios[\"urgency_level_score\"] = (\n",
    "    final_ratios[\"urgency_level_high_ratio\"] * 1 +\n",
    "    final_ratios[\"urgency_level_medium_ratio\"] * 0.3 +\n",
    "    final_ratios[\"urgency_level_low_ratio\"] * 0\n",
    ")\n",
    "\n",
    "# Display the updated dataframe with separate scores\n",
    "final_ratios[[\"userName\", \"signal_classification_score\", \"call_to_action_score\", \n",
    "              \"hype_classification_score\", \"urgency_level_score\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ratios[\"hype_classification_score\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crypto Manipulative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of False in the 'crypto_manipulative_words' column per user\n",
    "false_counts = tweet_analysis_merged.groupby(\"userName\")[\"crypto_manipulative_words\"].apply(lambda x: (x == False).sum()).reset_index()\n",
    "\n",
    "# Rename the column for clarity\n",
    "false_counts.rename(columns={\"crypto_manipulative_words\": \"false_count_crypto_manipulative_words\"}, inplace=True)\n",
    "\n",
    "# Normalize false_count_crypto_manipulative_words (Min-Max Scaling)\n",
    "min_val = false_counts[\"false_count_crypto_manipulative_words\"].min()\n",
    "max_val = false_counts[\"false_count_crypto_manipulative_words\"].max()\n",
    "\n",
    "# Avoid division by zero if all values are the same\n",
    "if min_val != max_val:\n",
    "    false_counts[\"normalized_false_count\"] = (false_counts[\"false_count_crypto_manipulative_words\"] - min_val) / (max_val - min_val)\n",
    "else:\n",
    "    false_counts[\"normalized_false_count\"] = 1  # If all values are the same, set them to 1\n",
    "\n",
    "# Display the first few rows\n",
    "false_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weightage System Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_df = pd.DataFrame()\n",
    "\n",
    "weighted_df[\"username\"] = author_stat_ratios_df[\"userName\"]\n",
    "weighted_df[\"data_driven_content\"] = author_stat_ratios_df[\"stat_vs_emot_ratio\"] * 0.6 + author_hist_ratios_df[\"historical_comparison_ratio\"] * 0.4\n",
    "weighted_df[\"signal_clarity\"] = author_hint_ratios_df[\"market_hint_ratio\"] * 0.3 + final_ratios[\"signal_classification_score\"] * 0.4 + final_ratios[\"call_to_action_score\"] * 0.3\n",
    "weighted_df[\"manipulative_resistance\"] = false_counts[\"normalized_false_count\"] * 0.5 + final_ratios[\"hype_classification_score\"] * 0.5\n",
    "weighted_df[\"urgency_sanity_check\"] = final_ratios['urgency_level_score']\n",
    "\n",
    "weighted_df[\"signal_quality\"] = weighted_df[\"data_driven_content\"] * 0.3 + weighted_df[\"signal_clarity\"] * 0.35 + weighted_df[\"manipulative_resistance\"] * 0.3 + weighted_df[\"urgency_sanity_check\"] * 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranking of Top 5 UserNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_df.sort_values(ascending=False,by=[\"signal_quality\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_df[\"signal_quality\"].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
