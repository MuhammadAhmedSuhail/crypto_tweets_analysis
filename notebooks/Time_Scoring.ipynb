{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pendle_tweets_df = pd.read_json(\"./dataset/pendle.json\")\n",
    "pendle_llm_analysis_df = pd.read_csv(\"./dataset/pendle_llm_analysis.csv\")\n",
    "pendle_coin_df = pd.read_csv(\"./dataset/pendle_ohlcv_30m.csv\")\n",
    "\n",
    "tenx_coins_df = pd.read_csv(\"./dataset/10x_coins.csv\")\n",
    "not_tenx_coins_df = pd.read_csv(\"./dataset/not_10x_coins.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pendle_tweets_df),len(pendle_llm_analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendle_tweets_df_non = pendle_tweets_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendle_tweets_df_non.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendle_tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendle_llm_analysis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendle_coin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "#  - Fetching All Coins Mentioned\n",
    "#  - Filtering out all Tweets to include atleast pendle in the coins_mentions column \n",
    "\n",
    "import ahocorasick\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba  # For Chinese text segmentation\n",
    "import unicodedata\n",
    "\n",
    "# Preprocess tenx_df and non_tenx_df: drop NaN values\n",
    "tenx_df = tenx_coins_df.dropna(subset=[\"name\", \"id\", \"symbol\", \"screen_name\"])\n",
    "non_tenx_df = not_tenx_coins_df.dropna(subset=[\"name\", \"id\", \"symbol\", \"screen_name\"])\n",
    "\n",
    "# Function to build Aho-Corasick trie\n",
    "def build_ac_trie(word_dict):\n",
    "    \"\"\"\n",
    "    Builds an Aho-Corasick Trie (automaton) for fast multi-pattern string matching.\n",
    "\n",
    "    The Aho-Corasick algorithm is an efficient method for searching multiple keywords \n",
    "    in a given text simultaneously. It constructs a Trie structure from a given dictionary \n",
    "    of keywords and then transforms it into an automaton that supports fast lookups.\n",
    "\n",
    "    Steps:\n",
    "    1. Insert each keyword from `word_dict` into the Trie.\n",
    "    2. Convert the Trie into an Aho-Corasick automaton with failure links, allowing \n",
    "       efficient backtracking when mismatches occur.\n",
    "    3. The resulting automaton enables linear-time matching of multiple words in a text.\n",
    "\n",
    "    Parameters:\n",
    "    word_dict (dict): A dictionary where keys are words (patterns to match) and values \n",
    "                      are associated identifiers.\n",
    "\n",
    "    Returns:\n",
    "    ahocorasick.Automaton: A compiled Aho-Corasick Trie ready for pattern matching.\n",
    "    \"\"\"\n",
    "    trie = ahocorasick.Automaton()\n",
    "    for key, value in word_dict.items():\n",
    "        trie.add_word(key, value)\n",
    "    trie.make_automaton()\n",
    "    return trie\n",
    "\n",
    "# Create mappings for tenx and non_tenx using 'name', 'symbol', and 'screen_name' columns\n",
    "name_to_id_10x = {k.lower(): v.lower() for k, v in zip(tenx_df[\"name\"], tenx_df[\"id\"])}\n",
    "symbol_to_id_10x = {k.lower(): v.lower() for k, v in zip(tenx_df[\"symbol\"], tenx_df[\"id\"])}\n",
    "screen_name_to_id_10x = {\"@\" + k.lower(): v.lower() for k, v in zip(tenx_df[\"screen_name\"], tenx_df[\"id\"])}\n",
    "\n",
    "name_to_id_non10x = {k.lower(): v.lower() for k, v in zip(non_tenx_df[\"name\"], non_tenx_df[\"id\"])}\n",
    "symbol_to_id_non10x = {k.lower(): v.lower() for k, v in zip(non_tenx_df[\"symbol\"], non_tenx_df[\"id\"])}\n",
    "screen_name_to_id_non10x = {\"@\" + k.lower(): v.lower() for k, v in zip(non_tenx_df[\"screen_name\"], non_tenx_df[\"id\"])}\n",
    "\n",
    "# Merge both datasets' mappings\n",
    "all_coin_mappings = {**name_to_id_10x, **symbol_to_id_10x, **screen_name_to_id_10x,\n",
    "                      **name_to_id_non10x, **symbol_to_id_non10x, **screen_name_to_id_non10x}\n",
    "\n",
    "# Common English words to filter out\n",
    "common_words = {\"about\", \"again\", \"all\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"bad\", \"be\", \"big\", \"but\", \"by\", \"can\", \n",
    "                \"different\", \"do\", \"early\", \"every\", \"for\", \"from\", \"good\", \"has\", \"high\", \"how\", \"if\", \"in\", \"is\", \"it\", \n",
    "                \"just\", \"late\", \"like\", \"long\", \"low\", \"me\", \"more\", \"most\", \"much\", \"my\", \"new\", \"not\", \"now\", \"old\", \"on\", \n",
    "                \"one\", \"only\", \"or\", \"other\", \"out\", \"over\", \"own\", \"short\", \"so\", \"that\", \"the\", \"this\", \"to\", \"under\", \"up\", \n",
    "                \"way\", \"we\", \"well\", \"what\", \"when\", \"where\", \"why\", \"will\", \"with\", \"would\", \"you\", \"young\", \"your\"}\n",
    "\n",
    "# Filter out short and ambiguous names\n",
    "filtered_mappings = {k: v for k, v in all_coin_mappings.items() if k not in common_words and len(k) > 2}\n",
    "\n",
    "# Build Aho-Corasick trie\n",
    "ac_trie = build_ac_trie(filtered_mappings)\n",
    "\n",
    "# Define regex patterns\n",
    "hashtag_pattern = re.compile(r'#([A-Za-z0-9]+)')\n",
    "mention_pattern = re.compile(r'@([A-Za-z0-9_]+)')\n",
    "dollar_pattern = re.compile(r'\\$([A-Za-z0-9]+)')\n",
    "\n",
    "# Function to detect non-English text\n",
    "def contains_non_english(text):\n",
    "    return any(unicodedata.category(char)[0] not in ('L', 'N') for char in text)\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize_text(text):\n",
    "    if contains_non_english(text):\n",
    "        return jieba.lcut(text)  # Use jieba for Chinese/Japanese text\n",
    "    else:\n",
    "        return re.findall(r'\\b[a-zA-Z0-9-]+\\b', text)  # Normal word extraction\n",
    "\n",
    "# Function to extract coin IDs from text\n",
    "def extract_coin_ids(text):\n",
    "    coin_ids = set()\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Extract hashtags\n",
    "    for match in hashtag_pattern.findall(text_lower):\n",
    "        if match in filtered_mappings:\n",
    "            coin_ids.add(filtered_mappings[match])\n",
    "\n",
    "    # Extract mentions\n",
    "    for match in mention_pattern.findall(text_lower):\n",
    "        match_lower = \"@\" + match\n",
    "        if match_lower in filtered_mappings:\n",
    "            coin_ids.add(filtered_mappings[match_lower])\n",
    "\n",
    "    # Extract tickers\n",
    "    for match in dollar_pattern.findall(text_lower):\n",
    "        if match in filtered_mappings:\n",
    "            coin_ids.add(filtered_mappings[match])\n",
    "\n",
    "    # Tokenize text based on language type\n",
    "    words = tokenize_text(text_lower)\n",
    "\n",
    "    # Use Aho-Corasick Trie to match words\n",
    "    for word in words:\n",
    "        if word in filtered_mappings:\n",
    "            coin_ids.add(filtered_mappings[word])\n",
    "\n",
    "    return list(coin_ids)\n",
    "\n",
    "# Apply function to extract mentions\n",
    "pendle_tweets_df[\"coin_mentions\"] = pendle_tweets_df[\"fullText\"].astype(str).apply(extract_coin_ids)\n",
    "\n",
    "# Ensure proper filtering of truly empty coin_mentions\n",
    "tweets_without_mentions = pendle_tweets_df[\n",
    "    pendle_tweets_df[\"coin_mentions\"].apply(lambda x: isinstance(x, list) and not x)\n",
    "]\n",
    "\n",
    "# Display sample results\n",
    "tweets_without_mentions[[\"id\", \"fullText\", \"coin_mentions\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pendle_tweets_df[\"coin_mentions\"]) , len(pendle_tweets_df[pendle_tweets_df[\"coin_mentions\"].apply(lambda x: \"pendle\" in x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting tweets where atleast pendle is mentioned\n",
    "pendle_mentioned_df = pendle_tweets_df[pendle_tweets_df[\"coin_mentions\"].apply(lambda x: \"pendle\" in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendle_mentioned_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the usernames from author column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract username from dictionary\n",
    "def extract_username(user_info):\n",
    "    try:\n",
    "        return user_info.get(\"userName\", None)  # Extract the username\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "# Apply extraction function to the username column\n",
    "pendle_tweets_df[\"username\"] = pendle_tweets_df[\"author\"].apply(extract_username)\n",
    "\n",
    "# Display the cleaned data\n",
    "pendle_tweets_df[[\"id\",\"fullText\",\"username\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract username from dictionary\n",
    "def extract_username(user_info):\n",
    "    try:\n",
    "        return user_info.get(\"userName\", None)  # Extract the username\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "# Apply extraction function to the username column\n",
    "pendle_mentioned_df[\"username\"] = pendle_mentioned_df[\"author\"].apply(extract_username)\n",
    "\n",
    "# Display the cleaned data\n",
    "pendle_mentioned_df[[\"id\",\"fullText\",\"username\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SubTask 1: For each account, find the time taken between the first mention of the coin, and the time taken by the coin to reach the 2x, 3x, and 5x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the first tweets of each user after storing out the createdAt\n",
    "pendle_mentioned_df[\"createdAt\"] = pd.to_datetime(pendle_mentioned_df[\"createdAt\"]).dt.tz_localize(None)\n",
    "\n",
    "# Group by username, sort by createdAt, and get the first tweet date per user\n",
    "first_tweets = pendle_mentioned_df.sort_values(\"createdAt\").groupby(\"username\").first().reset_index()\n",
    "\n",
    "# Extract only relevant columns\n",
    "first_tweets = first_tweets[[\"username\", \"createdAt\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tweets[\"rounded_timestamp\"] = first_tweets[\"createdAt\"].dt.floor(\"30T\")\n",
    "\n",
    "first_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendle_coin_df[\"TIMESTAMP\"] = pd.to_datetime(pendle_coin_df[\"TIMESTAMP\"]).dt.tz_localize(None)\n",
    "\n",
    "# Merge datasets again after fixing the timezone mismatch\n",
    "merged_data = first_tweets.merge(pendle_coin_df, left_on=\"rounded_timestamp\", right_on=\"TIMESTAMP\", how=\"left\")[[\"username\", \"createdAt\", \"rounded_timestamp\",\"CLOSE\"]]\n",
    "\n",
    "merged_data[\"CLOSE\"].fillna(0.4, inplace=True)\n",
    "\n",
    "# Display the final output\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ensure timestamps are in datetime format\n",
    "merged_data[\"createdAt\"] = pd.to_datetime(merged_data[\"createdAt\"])\n",
    "pendle_coin_df[\"TIMESTAMP\"] = pd.to_datetime(pendle_coin_df[\"TIMESTAMP\"])\n",
    "\n",
    "# Sort price data for efficient searching\n",
    "pendle_coin_df = pendle_coin_df.sort_values(by=[\"TIMESTAMP\"])\n",
    "\n",
    "# Convert to numpy arrays for fast vectorized operations\n",
    "timestamps = pendle_coin_df[\"TIMESTAMP\"].values\n",
    "prices = pendle_coin_df[\"CLOSE\"].values\n",
    "\n",
    "# Vectorized function to find days to reach 2x, 3x, and 5x\n",
    "def find_days_to_targets_fast(created_at, start_price):\n",
    "    target_prices = np.array([start_price * 2, start_price * 3, start_price * 5])  # 2x, 3x, 5x\n",
    "    idxs = np.searchsorted(prices, target_prices, side=\"left\")  # Find first occurrence\n",
    "\n",
    "    days = []\n",
    "    for idx in idxs:\n",
    "        if idx < len(timestamps):\n",
    "            time_diff = (timestamps[idx] - created_at).days\n",
    "            days.append(time_diff if time_diff >= 0 else None)  # Ensure no negatives\n",
    "        else:\n",
    "            days.append(0)  # No valid future price found\n",
    "\n",
    "    return days\n",
    "\n",
    "# Apply function efficiently\n",
    "merged_data[[\"Days_to_2x\", \"Days_to_3x\", \"Days_to_5x\"]] = np.vstack(\n",
    "    merged_data.apply(lambda row: find_days_to_targets_fast(row[\"rounded_timestamp\"], row[\"CLOSE\"]), axis=1)\n",
    ")\n",
    "\n",
    "# Display result\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"Days_to_2x\", \"Days_to_3x\", \"Days_to_5x\"]:\n",
    "    merged_data[col + \"_norm\"] = 1 - (merged_data[col] - merged_data[col].min()) / (merged_data[col].max() - merged_data[col].min())\n",
    "\n",
    "# Calculate the final weighted score\n",
    "merged_data[\"time_to_5x_movement\"] = (0.6 * merged_data[\"Days_to_5x_norm\"] + \n",
    "                     0.25 * merged_data[\"Days_to_3x_norm\"] + \n",
    "                     0.15 * merged_data[\"Days_to_2x_norm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendle_coin_df['TIMESTAMP'] = pd.to_datetime(pendle_coin_df['TIMESTAMP'])\n",
    "pendle_coin_df.set_index('TIMESTAMP', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure datetime index\n",
    "pendle_coin_df.index = pd.to_datetime(pendle_coin_df.index)\n",
    "\n",
    "# Calculate percentage changes for CLOSE and VOLUME\n",
    "pendle_coin_df['CLOSE_pct_change'] = pendle_coin_df['CLOSE'].pct_change() * 100\n",
    "pendle_coin_df['VOLUME_pct_change'] = pendle_coin_df['VOLUME'].pct_change() * 100\n",
    "\n",
    "# Compute relaxed surge thresholds (80th percentile)\n",
    "close_threshold_80 = pendle_coin_df['CLOSE_pct_change'].quantile(0.80)\n",
    "volume_threshold_80 = pendle_coin_df['VOLUME_pct_change'].quantile(0.80)\n",
    "\n",
    "# Identify surges using relaxed thresholds\n",
    "pendle_coin_df['surge'] = (\n",
    "    (pendle_coin_df['CLOSE_pct_change'] >= close_threshold_80) &\n",
    "    (pendle_coin_df['VOLUME_pct_change'] >= volume_threshold_80)\n",
    ")\n",
    "\n",
    "# Find contiguous surge periods\n",
    "pendle_coin_df['surge_shift'] = pendle_coin_df['surge'].shift(1, fill_value=False)\n",
    "pendle_coin_df['start'] = pendle_coin_df['surge'] & ~pendle_coin_df['surge_shift']  # Start of a new surge\n",
    "pendle_coin_df['end'] = ~pendle_coin_df['surge'] & pendle_coin_df['surge_shift']    # End of a surge\n",
    "\n",
    "# Extract start and end timestamps, ensuring minimum surge duration\n",
    "surge_periods = []\n",
    "start_time = None\n",
    "min_surge_duration = pd.Timedelta(hours=1)  # Minimum surge duration\n",
    "max_surge_duration = pd.Timedelta(hours=10)  # Maximum surge duration\n",
    "\n",
    "for timestamp, row in pendle_coin_df.iterrows():\n",
    "    if row['start']:\n",
    "        start_time = timestamp\n",
    "    if row['end'] and start_time is not None:\n",
    "        duration = timestamp - start_time\n",
    "        if duration >= min_surge_duration:\n",
    "            surge_periods.append((start_time, timestamp))\n",
    "        start_time = None\n",
    "\n",
    "# Handle case where last surge doesn't have an end yet\n",
    "if start_time is not None and pendle_coin_df['surge'].iloc[-1]:\n",
    "    end_time = pendle_coin_df.index[-1]\n",
    "    duration = end_time - start_time\n",
    "    if duration >= min_surge_duration:\n",
    "        surge_periods.append((start_time, end_time))\n",
    "\n",
    "# Print surge periods\n",
    "for start, end in surge_periods:\n",
    "    print(f\"Surge Period: {start.strftime('%Y-%m-%d %H:%M:%S')} to {end.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"Total Surge Periods:\", len(surge_periods))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SubTask2: Once you have found the surges, you need to calculate on average how much time before a surge, the account tweets about it. (Bullish sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding tweets that have a bullish sentiment\n",
    "bullish_only_tweets = pendle_llm_analysis_df[pendle_llm_analysis_df[\"signal_classification\"] == \"bullish\"]\n",
    "\n",
    "bullish_tweets_ids = bullish_only_tweets[\"id\"].to_list()\n",
    "\n",
    "bullish_tweets_df = pendle_mentioned_df[pendle_mentioned_df[\"id\"].isin(bullish_tweets_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bullish_tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'createdAt' to datetime and sort\n",
    "bullish_tweets_df['createdAt'] = pd.to_datetime(bullish_tweets_df['createdAt'])\n",
    "bullish_tweets_df = bullish_tweets_df.sort_values(by=['username', 'createdAt'])\n",
    "\n",
    "# Convert surge periods to DataFrame with proper datetime format\n",
    "surge_df = pd.DataFrame(surge_periods, columns=['surge_start', 'surge_end'])\n",
    "surge_df['surge_start'] = pd.to_datetime(surge_df['surge_start'])\n",
    "surge_df['surge_end'] = pd.to_datetime(surge_df['surge_end'])\n",
    "\n",
    "# Find the last tweet before each surge for each user using a more efficient approach\n",
    "results = []\n",
    "\n",
    "# Create a dictionary for quick lookup of last tweets before surge\n",
    "user_tweet_dict = {}\n",
    "for username, group in bullish_tweets_df.groupby('username'):\n",
    "    user_tweet_dict[username] = group['createdAt'].values\n",
    "\n",
    "# Process each surge period\n",
    "for _, surge_row in surge_df.iterrows():\n",
    "    surge_start = surge_row['surge_start']\n",
    "    \n",
    "    # Process each user for this surge period\n",
    "    for username, tweet_times in user_tweet_dict.items():\n",
    "        # Find last tweet before surge using numpy which is much faster\n",
    "        mask = tweet_times < surge_start\n",
    "        if mask.any():\n",
    "            last_tweet = tweet_times[mask].max()\n",
    "            days_before = (surge_start - pd.Timestamp(last_tweet)).days\n",
    "        else:\n",
    "            days_before = 0\n",
    "        \n",
    "        results.append({'username': username, 'days_before_surge': days_before})\n",
    "\n",
    "# Convert results to DataFrame and calculate average\n",
    "df_days = pd.DataFrame(results)\n",
    "avg_days_df = df_days.groupby('username', as_index=False)['days_before_surge'].mean()\n",
    "\n",
    "# Display the results\n",
    "avg_days_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_days_df[\"days_before_surge\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the LookBack window to be 3 days and then calculating the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set lookback window (3 days)\n",
    "LOOKBACK_HOURS = 72\n",
    "\n",
    "# Ensure createdAt is datetime\n",
    "bullish_tweets_df['createdAt'] = pd.to_datetime(bullish_tweets_df['createdAt'])\n",
    "\n",
    "# Convert surge periods to DataFrame\n",
    "surge_df = pd.DataFrame(surge_periods, columns=['surge_start', 'surge_end'])\n",
    "surge_df['surge_start'] = pd.to_datetime(surge_df['surge_start'])\n",
    "surge_df['surge_end'] = pd.to_datetime(surge_df['surge_end'])\n",
    "\n",
    "# Initialize lists to store results\n",
    "tweet_metrics = []\n",
    "\n",
    "# Calculate metrics for each surge\n",
    "for idx, surge in surge_df.iterrows():\n",
    "    lookback_start = surge['surge_start'] - pd.Timedelta(hours=LOOKBACK_HOURS)\n",
    "    \n",
    "    window_tweets = bullish_tweets_df[\n",
    "        ((bullish_tweets_df['createdAt'] >= lookback_start) & \n",
    "         (bullish_tweets_df['createdAt'] <= surge['surge_end']))\n",
    "    ]\n",
    "    \n",
    "    if window_tweets.empty:\n",
    "        continue\n",
    "        \n",
    "    for username, user_surge_tweets in window_tweets.groupby('username'):\n",
    "        user_surge_tweets['lead_time'] = (surge['surge_start'] - user_surge_tweets['createdAt']).dt.total_seconds() / (24*3600)\n",
    "        \n",
    "        pre_surge_tweets = user_surge_tweets[user_surge_tweets['createdAt'] < surge['surge_start']]\n",
    "        avg_lead_time = pre_surge_tweets['lead_time'].mean() if not pre_surge_tweets.empty else 0\n",
    "        \n",
    "        surge_mention = len(user_surge_tweets)\n",
    "        \n",
    "        interaction_cols = ['retweetCount', 'replyCount', 'likeCount', 'quoteCount', 'bookmarkCount']\n",
    "        user_surge_tweets['engagement_score'] = user_surge_tweets[interaction_cols].sum(axis=1) / user_surge_tweets['viewCount'].clip(lower=1)\n",
    "        avg_engagement = user_surge_tweets['engagement_score'].mean()\n",
    "        \n",
    "        tweet_metrics.append({\n",
    "            'username': username,\n",
    "            'lead_time': avg_lead_time,\n",
    "            'surge_mention': surge_mention,\n",
    "            'engagement_score': avg_engagement,\n",
    "            'surge_count': 1\n",
    "        })\n",
    "\n",
    "if not tweet_metrics:\n",
    "    print(\"No tweets found within lookback windows\")\n",
    "    result_df = pd.DataFrame(columns=['username', 'normalized_score'])\n",
    "else:\n",
    "    # Convert to DataFrame and aggregate by username\n",
    "    metrics_df = pd.DataFrame(tweet_metrics)\n",
    "    \n",
    "    # Aggregate metrics by username\n",
    "    user_metrics = metrics_df.groupby('username').agg({\n",
    "        'lead_time': 'mean',\n",
    "        'surge_mention': 'sum',\n",
    "        'engagement_score': 'mean',\n",
    "        'surge_count': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate conviction score\n",
    "    total_surges = len(surge_df)\n",
    "    user_metrics['conviction_score'] = user_metrics['surge_count'] / total_surges\n",
    "    \n",
    "    # Manual min-max normalization for each metric\n",
    "    norm_cols = ['lead_time', 'surge_mention', 'conviction_score', 'engagement_score']\n",
    "    \n",
    "    for col in norm_cols:\n",
    "        # Check if all values are the same\n",
    "        if user_metrics[col].min() == user_metrics[col].max():\n",
    "            user_metrics[f'normalized_{col}'] = 1.0\n",
    "        else:\n",
    "            # Min-max normalization: (x - min) / (max - min)\n",
    "            min_val = user_metrics[col].min()\n",
    "            max_val = user_metrics[col].max()\n",
    "            user_metrics[f'normalized_{col}'] = (user_metrics[col] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # # Calculate final combined score (equal weighting)\n",
    "    # normalized_cols = [f'normalized_{col}' for col in norm_cols]\n",
    "    # user_metrics['normalized_score'] = user_metrics[normalized_cols].mean(axis=1)\n",
    "    \n",
    "    # Final result with username and normalized score\n",
    "    result_df = user_metrics[['username', 'normalized_lead_time', 'normalized_surge_mention', 'normalized_conviction_score', 'normalized_engagement_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[\"lead_time_before_price_surges\"] = (\n",
    "    0.5 * result_df[\"normalized_lead_time\"] +\n",
    "    0.2 * result_df[\"normalized_surge_mention\"] +\n",
    "    0.15 * result_df[\"normalized_conviction_score\"] +\n",
    "    0.15 * result_df[\"normalized_engagement_score\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "early_detection Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating list of all unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pendle_tweets_df[\"username\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result_df[\"username\"]), len(merged_data[\"username\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Extract unique usernames from pendle_tweets_df\n",
    "unique_usernames = pendle_tweets_df[\"username\"].unique()\n",
    "\n",
    "# Step 2: Initialize dictionary with default values\n",
    "user_dict = {user: {\"time_to_5x_movement\": 0, \"lead_time_before_price_surges\": 0} for user in unique_usernames}\n",
    "\n",
    "# Step 3: Update the dictionary with values from merged_data\n",
    "for _, row in merged_data.iterrows():\n",
    "    username = row[\"username\"]\n",
    "    if username in user_dict:\n",
    "        user_dict[username][\"time_to_5x_movement\"] = row[\"time_to_5x_movement\"]\n",
    "\n",
    "# Step 4: Update the dictionary with values from results_df\n",
    "for _, row in result_df.iterrows():\n",
    "    username = row[\"username\"]\n",
    "    if username in user_dict:\n",
    "        user_dict[username][\"lead_time_before_price_surges\"] = row[\"lead_time_before_price_surges\"]\n",
    "\n",
    "# Step 5: Convert the dictionary to a DataFrame\n",
    "final_df = pd.DataFrame.from_dict(user_dict, orient=\"index\").reset_index()\n",
    "final_df.rename(columns={\"index\": \"username\"}, inplace=True)\n",
    "\n",
    "# Display the first few rows\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_df[\"username\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate early detection score\n",
    "final_df[\"early_detection\"] = (\n",
    "    final_df[\"time_to_5x_movement\"] * 0.5 +\n",
    "    final_df[\"lead_time_before_price_surges\"] * 0.5\n",
    ")\n",
    "\n",
    "# Sort in descending order based on early detection\n",
    "final_df = final_df.sort_values(by=\"early_detection\", ascending=False)\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweet Consistency Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendle_mentioned_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendle_mentioned_df[\"rounded_timestamp\"] = pendle_mentioned_df[\"createdAt\"].dt.floor(\"30T\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store tweet frequencies\n",
    "tweet_frequencies = []\n",
    "\n",
    "# Define lookback period for pre-surge tweets\n",
    "lookback_period = pd.Timedelta(hours=72)\n",
    "\n",
    "# Process each surge period\n",
    "for start, end in surge_periods:\n",
    "    # Filter tweets during surge\n",
    "    surge_tweets = pendle_mentioned_df[(pendle_mentioned_df['createdAt'] >= start) & (pendle_mentioned_df['createdAt'] <= end)]\n",
    "    surge_counts = surge_tweets.groupby(\"username\").size().reset_index(name=\"insurge_tweets_frequency\")\n",
    "    \n",
    "    # Filter tweets 72 hours before surge\n",
    "    pre_surge_tweets = pendle_mentioned_df[(pendle_mentioned_df['createdAt'] >= start - lookback_period) & (pendle_mentioned_df['createdAt'] < start)]\n",
    "    pre_surge_counts = pre_surge_tweets.groupby(\"username\").size().reset_index(name=\"presurge_tweets_frequency\")\n",
    "    \n",
    "    # Merge results to ensure all usernames are considered\n",
    "    tweet_frequency = pd.merge(pre_surge_counts, surge_counts, on=\"username\", how=\"outer\").fillna(0)\n",
    "    \n",
    "    # Append results\n",
    "    tweet_frequencies.append(tweet_frequency)\n",
    "\n",
    "# Combine results into a single DataFrame\n",
    "tweet_frequency_result = pd.concat(tweet_frequencies, ignore_index=True)\n",
    "\n",
    "# **Aggregate by username to get total tweets across all surge periods**\n",
    "tweet_frequency_result = tweet_frequency_result.groupby(\"username\", as_index=False).sum()\n",
    "\n",
    "# Sort by in-surge tweet frequency\n",
    "tweet_frequency_result.sort_values(by=\"insurge_tweets_frequency\", ascending=False, inplace=True)\n",
    "\n",
    "tweet_frequency_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buy Tweets Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lookback period for pre-surge tweets\n",
    "lookback_period = pd.Timedelta(hours=72)\n",
    "\n",
    "# Getting IDs of all Buy tweets\n",
    "buy_tweets_ids = pendle_llm_analysis_df[pendle_llm_analysis_df[\"call_to_action\"] == \"buy\"][\"id\"].to_list()\n",
    "\n",
    "# Create an empty list to store buy tweet ratios\n",
    "buy_ratios = []\n",
    "\n",
    "# Function to compute buy tweet ratio\n",
    "def compute_buy_tweet_ratio(df):\n",
    "    total_tweets = df.groupby(\"username\")[\"id\"].count()\n",
    "    buy_tweets = df[df[\"id\"].isin(buy_tweets_ids)].groupby(\"username\")[\"id\"].count()\n",
    "    return (buy_tweets / total_tweets).fillna(0)\n",
    "\n",
    "# Temporary storage for merging all results\n",
    "all_pre_surge_ratios = []\n",
    "all_in_surge_ratios = []\n",
    "\n",
    "# Calculate buy tweet ratio before and during surges\n",
    "for start, end in surge_periods:\n",
    "    # Filter tweets during surge\n",
    "    surge_tweets = pendle_mentioned_df[(pendle_mentioned_df['createdAt'] >= start) & (pendle_mentioned_df['createdAt'] <= end)]\n",
    "    \n",
    "    # Filter tweets 72 hours before surge\n",
    "    pre_surge_tweets = pendle_mentioned_df[(pendle_mentioned_df['createdAt'] >= start - lookback_period) & (pendle_mentioned_df['createdAt'] < start)]\n",
    "    \n",
    "    # Compute buy tweet ratios\n",
    "    pre_surge_buy_ratio = compute_buy_tweet_ratio(pre_surge_tweets)\n",
    "    in_surge_buy_ratio = compute_buy_tweet_ratio(surge_tweets)\n",
    "    \n",
    "    # Store results for later aggregation\n",
    "    all_pre_surge_ratios.append(pre_surge_buy_ratio)\n",
    "    all_in_surge_ratios.append(in_surge_buy_ratio)\n",
    "\n",
    "# Combine all periods and sum them across users\n",
    "pre_surge_buy_ratios = pd.concat(all_pre_surge_ratios, axis=1).sum(axis=1)\n",
    "in_surge_buy_ratios = pd.concat(all_in_surge_ratios, axis=1).sum(axis=1)\n",
    "\n",
    "# Merge into final DataFrame\n",
    "buy_ratio_result = pd.DataFrame({\n",
    "    \"username\": pre_surge_buy_ratios.index.union(in_surge_buy_ratios.index),\n",
    "    \"presurge_buy_tweets_ratio\": pre_surge_buy_ratios,\n",
    "    \"insurge_buy_tweets_ratio\": in_surge_buy_ratios\n",
    "}).fillna(0)\n",
    "\n",
    "# Sort by In-Surge Buy Tweets Ratio (highest first)\n",
    "buy_ratio_result = buy_ratio_result.sort_values(by=\"insurge_buy_tweets_ratio\", ascending=False)\n",
    "buy_ratio_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "buy_ratio_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_ratio_result[\"presurge_buy_tweets_ratio\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bullish Tweets Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lookback period for pre-surge tweets\n",
    "lookback_period = pd.Timedelta(hours=72)\n",
    "\n",
    "# Getting IDs of all Bullish tweets\n",
    "bullish_tweets_ids = pendle_llm_analysis_df[pendle_llm_analysis_df[\"signal_classification\"] == \"bullish\"][\"id\"].to_list()\n",
    "\n",
    "# Function to compute bullish tweet ratio\n",
    "def compute_bullish_tweet_ratio(df):\n",
    "    total_tweets = df.groupby(\"username\")[\"id\"].count()\n",
    "    bullish_tweets = df[df[\"id\"].isin(bullish_tweets_ids)].groupby(\"username\")[\"id\"].count()\n",
    "    return (bullish_tweets / total_tweets).fillna(0)\n",
    "\n",
    "# Temporary storage for merging all results\n",
    "all_pre_surge_ratios = []\n",
    "all_in_surge_ratios = []\n",
    "\n",
    "# Calculate bullish tweet ratio before and during surges\n",
    "for start, end in surge_periods:\n",
    "    # Filter tweets during surge\n",
    "    surge_tweets = pendle_mentioned_df[(pendle_mentioned_df['createdAt'] >= start) & (pendle_mentioned_df['createdAt'] <= end)]\n",
    "    \n",
    "    # Filter tweets 72 hours before surge\n",
    "    pre_surge_tweets = pendle_mentioned_df[(pendle_mentioned_df['createdAt'] >= start - lookback_period) & (pendle_mentioned_df['createdAt'] < start)]\n",
    "    \n",
    "    # Compute bullish tweet ratios\n",
    "    pre_surge_bullish_ratio = compute_bullish_tweet_ratio(pre_surge_tweets)\n",
    "    in_surge_bullish_ratio = compute_bullish_tweet_ratio(surge_tweets)\n",
    "    \n",
    "    # Store results for later aggregation\n",
    "    all_pre_surge_ratios.append(pre_surge_bullish_ratio)\n",
    "    all_in_surge_ratios.append(in_surge_bullish_ratio)\n",
    "\n",
    "# Combine all periods and sum them across users\n",
    "pre_surge_bullish_ratios = pd.concat(all_pre_surge_ratios, axis=1).sum(axis=1)\n",
    "in_surge_bullish_ratios = pd.concat(all_in_surge_ratios, axis=1).sum(axis=1)\n",
    "\n",
    "# Merge into final DataFrame\n",
    "bullish_ratio_result = pd.DataFrame({\n",
    "    \"username\": pre_surge_bullish_ratios.index.union(in_surge_bullish_ratios.index),\n",
    "    \"presurge_bullish_tweets_ratio\": pre_surge_bullish_ratios,\n",
    "    \"insurge_bullish_tweets_ratio\": in_surge_bullish_ratios\n",
    "}).fillna(0)\n",
    "\n",
    "# Sort by In-Surge Bullish Tweets Ratio (highest first)\n",
    "bullish_ratio_result = bullish_ratio_result.sort_values(by=\"insurge_bullish_tweets_ratio\", ascending=False)\n",
    "bullish_ratio_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "bullish_ratio_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hype Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_analysis_merged_df = pendle_mentioned_df.merge(pendle_llm_analysis_df, on=\"id\", how=\"left\")  # 'left' keeps all records in df\n",
    "\n",
    "# Now df contains the analysis for each tweet\n",
    "tweets_analysis_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_analysis_merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_analysis_merged_df['in_surge_period'] = tweets_analysis_merged_df['createdAt'].apply(lambda x: any(start <= x <= end for start, end in surge_periods))\n",
    "surge_tweets = tweets_analysis_merged_df[tweets_analysis_merged_df['in_surge_period']]\n",
    "non_surge_tweets = tweets_analysis_merged_df[~tweets_analysis_merged_df['in_surge_period']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scoring criteria\n",
    "hype_scores = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "\n",
    "# Map hype classification to numerical scores\n",
    "surge_tweets[\"hype_score\"] = surge_tweets[\"hype_classification\"].map(hype_scores)\n",
    "\n",
    "# Group by username and sum the hype scores\n",
    "final_hype_scores = surge_tweets.groupby(\"username\")[\"hype_score\"].sum().reset_index()\n",
    "\n",
    "# Sort the results from highest to lowest\n",
    "final_hype_scores = final_hype_scores.sort_values(by=\"hype_score\", ascending=False)\n",
    "\n",
    "# Display the top results\n",
    "final_hype_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scoring criteria\n",
    "hype_scores = {\"low\": 0, \"normal\": 1, \"high\": 2}\n",
    "\n",
    "# Map hype classification to numerical scores\n",
    "non_surge_tweets[\"hype_score\"] = non_surge_tweets[\"hype_classification\"].map(hype_scores)\n",
    "\n",
    "# Group by username and sum the hype scores\n",
    "final_hype_non_surge = non_surge_tweets.groupby(\"username\")[\"hype_score\"].sum().reset_index()\n",
    "\n",
    "# Sort the results from highest to lowest\n",
    "final_hype_non_surge = final_hype_non_surge.sort_values(by=\"hype_score\", ascending=False)\n",
    "\n",
    "# Display the top results\n",
    "final_hype_non_surge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulative Tweets Ratio in Surge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total tweets and manipulative tweets per username\n",
    "manipulative_ratio = surge_tweets.groupby(\"username\").agg(\n",
    "    total_tweets=(\"id\", \"count\"),\n",
    "    manipulative_tweets=(\"crypto_manipulative_words\", \"sum\")\n",
    ")\n",
    "\n",
    "# Compute the ratio\n",
    "manipulative_ratio[\"manipulative_tweets_ratio\"] = (\n",
    "    manipulative_ratio[\"manipulative_tweets\"] / manipulative_ratio[\"total_tweets\"]\n",
    ")\n",
    "\n",
    "# Sort in descending order\n",
    "manipulative_ratio = manipulative_ratio.sort_values(by=\"manipulative_tweets_ratio\", ascending=False)\n",
    "manipulative_ratio = manipulative_ratio[[\"manipulative_tweets_ratio\"]].reset_index()\n",
    "\n",
    "# Display results\n",
    "manipulative_ratio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total tweets and manipulative tweets per username\n",
    "manipulative_ratio_non_surge = non_surge_tweets.groupby(\"username\").agg(\n",
    "    total_tweets=(\"id\", \"count\"),\n",
    "    manipulative_tweets=(\"crypto_manipulative_words\", \"sum\")\n",
    ")\n",
    "\n",
    "# Compute the ratio\n",
    "manipulative_ratio_non_surge[\"manipulative_tweets_ratio\"] = (\n",
    "    manipulative_ratio_non_surge[\"manipulative_tweets\"] / manipulative_ratio_non_surge[\"total_tweets\"]\n",
    ")\n",
    "\n",
    "# Sort in descending order\n",
    "manipulative_ratio_non_surge = manipulative_ratio_non_surge.sort_values(by=\"manipulative_tweets_ratio\", ascending=False)\n",
    "manipulative_ratio_non_surge = manipulative_ratio_non_surge[[\"manipulative_tweets_ratio\"]].reset_index()\n",
    "\n",
    "# Display results\n",
    "manipulative_ratio_non_surge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique_users = pendle_tweets_df[\"username\"].unique()\n",
    "\n",
    "len(all_unique_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reference dataframe from unique users\n",
    "unique_users_df = pd.DataFrame({\"username\": all_unique_users})  # Ensure this is a complete list\n",
    "\n",
    "# List of dataframes to merge\n",
    "dfs = [tweet_frequency_result, buy_ratio_result, bullish_ratio_result, final_hype_scores, manipulative_ratio]\n",
    "\n",
    "# # Start merging from the unique users dataframe\n",
    "# merged_df = reduce(lambda left, right: pd.merge(left, right, on=\"username\", how=\"left\"), [unique_users_df] + dfs)\n",
    "\n",
    "merged_df = unique_users_df.copy()\n",
    "for df in dfs:\n",
    "    merged_df = pd.merge(merged_df, df, on=\"username\", how=\"left\")\n",
    "\n",
    "\n",
    "# Fill missing values with 0\n",
    "merged_df = merged_df.fillna(0)\n",
    "\n",
    "# Display the final result\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_users_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_unique_users), len(merged_df[\"username\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns for normalization\n",
    "numeric_columns = merged_df.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "# Apply Min-Max Scaling manually\n",
    "for col in numeric_columns:\n",
    "    min_val = merged_df[col].min()\n",
    "    max_val = merged_df[col].max()\n",
    "    if max_val != min_val:  # Avoid division by zero\n",
    "        merged_df[col] = (merged_df[col] - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        merged_df[col] = 0  # If all values are the same, set column to 0\n",
    "\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"consistency_score\"] = (\n",
    "    merged_df[\"presurge_buy_tweets_ratio\"] * 0.25 +\n",
    "    merged_df[\"insurge_tweets_frequency\"] * 0.1 +\n",
    "    merged_df[\"presurge_bullish_tweets_ratio\"] * 0.1 +\n",
    "    merged_df[\"insurge_bullish_tweets_ratio\"] * 0.05 +\n",
    "    merged_df[\"presurge_tweets_frequency\"] * 0.1 +\n",
    "    merged_df[\"insurge_buy_tweets_ratio\"] * 0.1 +\n",
    "    merged_df[\"hype_score\"] * 0.15 +\n",
    "    merged_df[\"manipulative_tweets_ratio\"] * 0.15\n",
    ")\n",
    "\n",
    "merged_df.sort_values(by=\"consistency_score\",ascending=False,inplace=True)\n",
    "\n",
    "merged_df[[\"username\",\"consistency_score\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.sort_values(by=\"username\",ascending=True, inplace=True)\n",
    "final_df.sort_values(by=\"username\",ascending=True, inplace=True)\n",
    "\n",
    "timing_and_relevance = pd.DataFrame()\n",
    "\n",
    "timing_and_relevance[\"username\"] = merged_df[\"username\"]\n",
    "timing_and_relevance[\"score\"] = merged_df[\"consistency_score\"] * 0.4 + final_df[\"early_detection\"] * 0.6\n",
    "\n",
    "timing_and_relevance.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "\n",
    "timing_and_relevance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(timing_and_relevance[\"username\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
