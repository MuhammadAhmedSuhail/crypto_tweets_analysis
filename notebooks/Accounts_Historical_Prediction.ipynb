{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pendle_df = pd.read_json('./pendle.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendle_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_only_tweets = pendle_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendle_llm_analysis = pd.read_csv(\"./pendle_llm_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendle_llm_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_only_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching userName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Function to extract username from dictionary\n",
    "def extract_username(user_info):\n",
    "    try:\n",
    "        return user_info.get(\"userName\", None)  # Extract the username\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "# Apply extraction function to the username column\n",
    "prediction_only_tweets[\"username\"] = prediction_only_tweets[\"author\"].apply(extract_username)\n",
    "\n",
    "# Display the cleaned data\n",
    "prediction_only_tweets[[\"id\",\"fullText\",\"username\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenx_df = pd.read_csv(\"10x_coins.csv\")\n",
    "\n",
    "tenx_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_tenx_df = pd.read_csv(\"not_10x_coins.csv\")\n",
    "\n",
    "non_tenx_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ahocorasick\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba  # For Chinese text segmentation\n",
    "import unicodedata\n",
    "\n",
    "# Preprocess tenx_df and non_tenx_df: drop NaN values\n",
    "tenx_df = tenx_df.dropna(subset=[\"name\", \"id\", \"symbol\", \"screen_name\"])\n",
    "non_tenx_df = non_tenx_df.dropna(subset=[\"name\", \"id\", \"symbol\", \"screen_name\"])\n",
    "\n",
    "# Function to build Aho-Corasick trie\n",
    "def build_ac_trie(word_dict):\n",
    "    \"\"\"\n",
    "    Builds an Aho-Corasick Trie (automaton) for fast multi-pattern string matching.\n",
    "\n",
    "    The Aho-Corasick algorithm is an efficient method for searching multiple keywords \n",
    "    in a given text simultaneously. It constructs a Trie structure from a given dictionary \n",
    "    of keywords and then transforms it into an automaton that supports fast lookups.\n",
    "\n",
    "    Steps:\n",
    "    1. Insert each keyword from `word_dict` into the Trie.\n",
    "    2. Convert the Trie into an Aho-Corasick automaton with failure links, allowing \n",
    "       efficient backtracking when mismatches occur.\n",
    "    3. The resulting automaton enables linear-time matching of multiple words in a text.\n",
    "\n",
    "    Parameters:\n",
    "    word_dict (dict): A dictionary where keys are words (patterns to match) and values \n",
    "                      are associated identifiers.\n",
    "\n",
    "    Returns:\n",
    "    ahocorasick.Automaton: A compiled Aho-Corasick Trie ready for pattern matching.\n",
    "    \"\"\"\n",
    "    trie = ahocorasick.Automaton()\n",
    "    for key, value in word_dict.items():\n",
    "        trie.add_word(key, value)\n",
    "    trie.make_automaton()\n",
    "    return trie\n",
    "\n",
    "# Create mappings for tenx and non_tenx using 'name', 'symbol', and 'screen_name' columns\n",
    "name_to_id_10x = {k.lower(): v.lower() for k, v in zip(tenx_df[\"name\"], tenx_df[\"id\"])}\n",
    "symbol_to_id_10x = {k.lower(): v.lower() for k, v in zip(tenx_df[\"symbol\"], tenx_df[\"id\"])}\n",
    "screen_name_to_id_10x = {\"@\" + k.lower(): v.lower() for k, v in zip(tenx_df[\"screen_name\"], tenx_df[\"id\"])}\n",
    "\n",
    "name_to_id_non10x = {k.lower(): v.lower() for k, v in zip(non_tenx_df[\"name\"], non_tenx_df[\"id\"])}\n",
    "symbol_to_id_non10x = {k.lower(): v.lower() for k, v in zip(non_tenx_df[\"symbol\"], non_tenx_df[\"id\"])}\n",
    "screen_name_to_id_non10x = {\"@\" + k.lower(): v.lower() for k, v in zip(non_tenx_df[\"screen_name\"], non_tenx_df[\"id\"])}\n",
    "\n",
    "# Merge both datasets' mappings\n",
    "all_coin_mappings = {**name_to_id_10x, **symbol_to_id_10x, **screen_name_to_id_10x,\n",
    "                      **name_to_id_non10x, **symbol_to_id_non10x, **screen_name_to_id_non10x}\n",
    "\n",
    "# Common English words to filter out\n",
    "common_words = {\"about\", \"again\", \"all\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"bad\", \"be\", \"big\", \"but\", \"by\", \"can\", \n",
    "                \"different\", \"do\", \"early\", \"every\", \"for\", \"from\", \"good\", \"has\", \"high\", \"how\", \"if\", \"in\", \"is\", \"it\", \n",
    "                \"just\", \"late\", \"like\", \"long\", \"low\", \"me\", \"more\", \"most\", \"much\", \"my\", \"new\", \"not\", \"now\", \"old\", \"on\", \n",
    "                \"one\", \"only\", \"or\", \"other\", \"out\", \"over\", \"own\", \"short\", \"so\", \"that\", \"the\", \"this\", \"to\", \"under\", \"up\", \n",
    "                \"way\", \"we\", \"well\", \"what\", \"when\", \"where\", \"why\", \"will\", \"with\", \"would\", \"you\", \"young\", \"your\"}\n",
    "\n",
    "# Filter out short and ambiguous names\n",
    "filtered_mappings = {k: v for k, v in all_coin_mappings.items() if k not in common_words and len(k) > 2}\n",
    "\n",
    "# Build Aho-Corasick trie\n",
    "ac_trie = build_ac_trie(filtered_mappings)\n",
    "\n",
    "# Define regex patterns\n",
    "hashtag_pattern = re.compile(r'#([A-Za-z0-9]+)')\n",
    "mention_pattern = re.compile(r'@([A-Za-z0-9_]+)')\n",
    "dollar_pattern = re.compile(r'\\$([A-Za-z0-9]+)')\n",
    "\n",
    "# Function to detect non-English text\n",
    "def contains_non_english(text):\n",
    "    return any(unicodedata.category(char)[0] not in ('L', 'N') for char in text)\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize_text(text):\n",
    "    if contains_non_english(text):\n",
    "        return jieba.lcut(text)  # Use jieba for Chinese/Japanese text\n",
    "    else:\n",
    "        return re.findall(r'\\b[a-zA-Z0-9-]+\\b', text)  # Normal word extraction\n",
    "\n",
    "# Function to extract coin IDs from text\n",
    "def extract_coin_ids(text):\n",
    "    coin_ids = set()\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Extract hashtags\n",
    "    for match in hashtag_pattern.findall(text_lower):\n",
    "        if match in filtered_mappings:\n",
    "            coin_ids.add(filtered_mappings[match])\n",
    "\n",
    "    # Extract mentions\n",
    "    for match in mention_pattern.findall(text_lower):\n",
    "        match_lower = \"@\" + match\n",
    "        if match_lower in filtered_mappings:\n",
    "            coin_ids.add(filtered_mappings[match_lower])\n",
    "\n",
    "    # Extract tickers\n",
    "    for match in dollar_pattern.findall(text_lower):\n",
    "        if match in filtered_mappings:\n",
    "            coin_ids.add(filtered_mappings[match])\n",
    "\n",
    "    # Tokenize text based on language type\n",
    "    words = tokenize_text(text_lower)\n",
    "\n",
    "    # Use Aho-Corasick Trie to match words\n",
    "    for word in words:\n",
    "        if word in filtered_mappings:\n",
    "            coin_ids.add(filtered_mappings[word])\n",
    "\n",
    "    return list(coin_ids)\n",
    "\n",
    "# Apply function to extract mentions\n",
    "prediction_only_tweets[\"coin_mentions\"] = prediction_only_tweets[\"fullText\"].astype(str).apply(extract_coin_ids)\n",
    "\n",
    "# Ensure proper filtering of truly empty coin_mentions\n",
    "tweets_without_mentions = prediction_only_tweets[\n",
    "    prediction_only_tweets[\"coin_mentions\"].apply(lambda x: isinstance(x, list) and not x)\n",
    "]\n",
    "\n",
    "# Display sample results\n",
    "tweets_without_mentions[[\"id\", \"fullText\", \"coin_mentions\"]].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_only_tweets.iloc[5345][\"fullText\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_only_tweets.iloc[5345][\"coin_mentions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Prediction Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamps to datetime objects\n",
    "prediction_only_tweets['tweet_date'] = pd.to_datetime(prediction_only_tweets['createdAt'], errors='coerce')\n",
    "tenx_df['all_time_high_date'] = pd.to_datetime(tenx_df['ath_date'], errors='coerce')\n",
    "\n",
    "# Create a dictionary of coin name to ATH date for faster lookup\n",
    "coin_ath_dict = dict(zip(tenx_df['name'].str.lower(), tenx_df['all_time_high_date']))\n",
    "\n",
    "# Function to analyze predictions\n",
    "def analyze_predictions(row):\n",
    "    try:\n",
    "        if isinstance(row['coin_mentions'], list):\n",
    "            coins = row['coin_mentions']\n",
    "        else:\n",
    "            coins = ast.literal_eval(row['coin_mentions'])\n",
    "        \n",
    "        # Count total predictions\n",
    "        total_predictions = len(coins)\n",
    "        \n",
    "        # Count successful predictions (coins mentioned before ATH)\n",
    "        successful_predictions = sum(\n",
    "            1 for coin in coins \n",
    "            if row['tweet_date'] < coin_ath_dict.get(coin.lower(), pd.NaT)\n",
    "        )\n",
    "        \n",
    "        return pd.Series([total_predictions, successful_predictions])\n",
    "    except Exception:\n",
    "        return pd.Series([0, 0])\n",
    "\n",
    "# Apply the function to create new columns\n",
    "prediction_only_tweets[['total_predictions', 'successful_predictions']] = prediction_only_tweets.apply(analyze_predictions, axis=1)\n",
    "\n",
    "prediction_only_tweets[['id', 'coin_mentions', 'total_predictions', 'successful_predictions']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_only_tweets[prediction_only_tweets[\"lang\"] != \"en\"][[\"fullText\",\"coin_mentions\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successful 10x prediction ratio per userName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by userName and calculate statistics\n",
    "user_stats = prediction_only_tweets.groupby('username').agg({\n",
    "    'total_predictions': 'sum',\n",
    "    'successful_predictions': 'sum',\n",
    "    'id': 'count'  # Count of tweets per user\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate success ratio\n",
    "user_stats['success_ratio'] = user_stats['successful_predictions'] * 2 / user_stats['total_predictions'] * 5\n",
    "user_stats['successful_10x_predictions_ratio'] = user_stats['success_ratio'].fillna(0)  # Handle division by zero\n",
    "\n",
    "# Rename columns for clarity\n",
    "user_stats = user_stats.rename(columns={'id': 'tweet_count'})\n",
    "\n",
    "# Sort by success ratio in descending order\n",
    "user_stats = user_stats.sort_values('successful_10x_predictions_ratio', ascending=False)\n",
    "\n",
    "user_stats[[\"username\",\"successful_10x_predictions_ratio\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats[\"successful_10x_predictions_ratio\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the longest tweets streak by each handle and average time span in days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_only_tweets[\"successful_predictions\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tweets = prediction_only_tweets.copy()\n",
    "\n",
    "df = pred_tweets\n",
    "\n",
    "# Convert createdAt to datetime\n",
    "df['createdAt'] = pd.to_datetime(df['createdAt'])\n",
    "\n",
    "# Step 1: Group by username and sort by createdAt\n",
    "df = df.sort_values(['username', 'createdAt'])\n",
    "\n",
    "# Step 2: Identify streaks using cumsum\n",
    "df['streak_id'] = df.groupby('username')['successful_predictions'].apply(lambda x: (x == 0).cumsum())\n",
    "\n",
    "# Filter out zeros (breaking points)\n",
    "streaks = df[df['successful_predictions'] != 0].groupby(['username', 'streak_id'])\n",
    "\n",
    "# Calculate longest streak and average time span\n",
    "streak_lengths = streaks['successful_predictions'].count().reset_index(name='streak_length')\n",
    "max_streaks = streak_lengths.groupby('username')['streak_length'].max().reset_index(name='longest_tweets_streak')\n",
    "\n",
    "time_spans = (streaks['createdAt'].max() - streaks['createdAt'].min()).dt.total_seconds() / (3600 * 24)\n",
    "avg_time_spans = time_spans.groupby('username').mean().reset_index(name='avg_time_span_days')\n",
    "\n",
    "# Merge results\n",
    "result_df = pd.merge(max_streaks, avg_time_spans, on='username', how='left')\n",
    "\n",
    "result_df.sort_values(\"longest_tweets_streak\",ascending=False,inplace=True)\n",
    "\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplified Version of Streak IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Copy the dataset\n",
    "df = prediction_only_tweets.copy()\n",
    "\n",
    "# Step 1: Convert 'createdAt' to datetime\n",
    "df['createdAt'] = pd.to_datetime(df['createdAt'])\n",
    "\n",
    "# Step 2: Sort by 'username' and 'createdAt'\n",
    "df = df.sort_values(['username', 'createdAt']).reset_index(drop=True)\n",
    "\n",
    "# Step 3: Calculate longest streak per username\n",
    "longest_streaks = {}\n",
    "avg_time_spans = {}\n",
    "\n",
    "for username in df['username'].unique():\n",
    "    user_df = df[df['username'] == username]\n",
    "    \n",
    "    longest_streak = 0\n",
    "    current_streak = 0\n",
    "    streak_start_time = None\n",
    "    streak_durations = []\n",
    "\n",
    "    for i in range(len(user_df)):\n",
    "        row = user_df.iloc[i]\n",
    "        \n",
    "        if row['successful_predictions'] != 0:\n",
    "            if current_streak == 0:\n",
    "                streak_start_time = row['createdAt']\n",
    "            current_streak += 1\n",
    "        else:\n",
    "            if current_streak > 0:\n",
    "                longest_streak = max(longest_streak, current_streak)\n",
    "                if streak_start_time is not None:\n",
    "                    streak_durations.append((row['createdAt'] - streak_start_time).total_seconds() / (3600 * 24))\n",
    "                current_streak = 0\n",
    "                streak_start_time = None\n",
    "    \n",
    "    # Final check at end of loop\n",
    "    if current_streak > 0:\n",
    "        longest_streak = max(longest_streak, current_streak)\n",
    "        if streak_start_time is not None:\n",
    "            streak_durations.append((user_df.iloc[-1]['createdAt'] - streak_start_time).total_seconds() / (3600 * 24))\n",
    "    \n",
    "    longest_streaks[username] = longest_streak\n",
    "    avg_time_spans[username] = sum(streak_durations) / len(streak_durations) if streak_durations else 0\n",
    "\n",
    "# Convert results to DataFrame\n",
    "streak_df = pd.DataFrame(list(longest_streaks.items()), columns=['username', 'longest_tweets_streak'])\n",
    "time_span_df = pd.DataFrame(list(avg_time_spans.items()), columns=['username', 'avg_time_span_days'])\n",
    "\n",
    "# Merge and sort final results\n",
    "one_result_df = pd.merge(streak_df, time_span_df, on='username', how='left')\n",
    "one_result_df.sort_values('longest_tweets_streak', ascending=False, inplace=True)\n",
    "\n",
    "one_result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condition Part-1 (call_to_action is written as buy but no before_ath_coin is mentioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_tweets_analysis = pendle_llm_analysis.copy()\n",
    "\n",
    "buy_tweets_analysis[ (buy_tweets_analysis[\"call_to_action\"] == \"buy\") ].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_tweets_ids = buy_tweets_analysis[\"id\"].to_list()\n",
    "\n",
    "buy_tweets_df = prediction_only_tweets[prediction_only_tweets[\"id\"].isin(buy_tweets_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_tweets_df[\"successful_predictions\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_tweets_df = buy_tweets_df[buy_tweets_df[\"successful_predictions\"] == 0]\n",
    "\n",
    "buy_tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condition Part-2 (tweet has a bullish signal, but no coin is mentioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bullish_tweets_analysis = pendle_llm_analysis.copy()\n",
    "\n",
    "bullish_tweets_analysis[ (bullish_tweets_analysis[\"signal_classification\"] == \"bullish\") ].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bullish_tweets_ids = bullish_tweets_analysis[\"id\"].to_list()\n",
    "\n",
    "bullish_tweets_df = prediction_only_tweets[prediction_only_tweets[\"id\"].isin(bullish_tweets_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bullish_tweets_df = bullish_tweets_df[bullish_tweets_df[\"coin_mentions\"].apply(lambda x: len(x) == 0)]\n",
    "bullish_tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating DataFrames from both condition parts (Part1+Part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bullish_tweets_df),len(buy_tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_buy_df = pd.concat([buy_tweets_df, bullish_tweets_df])\n",
    "\n",
    "incorrect_buy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Prediction Success Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats.sort_values(\"username\",ascending=True,inplace=True)\n",
    "result_df.sort_values(\"username\",ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_usernames = set(user_stats[\"username\"]) - set(result_df[\"username\"])\n",
    "print(missing_usernames)\n",
    "\n",
    "# Create a DataFrame with missing users and default values\n",
    "missing_users_df = user_stats[user_stats[\"username\"].isin(missing_usernames)].copy()\n",
    "missing_users_df[\"longest_tweets_streak\"] = 0\n",
    "missing_users_df[\"avg_time_span_days\"] = 0\n",
    "\n",
    "# Append missing users to result_df\n",
    "result_df = pd.concat([result_df, missing_users_df], ignore_index=True)\n",
    "\n",
    "len(result_df[\"username\"]),len(user_stats[\"username\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid division by zero by checking if all values are the same\n",
    "min_val = user_stats[\"successful_10x_predictions_ratio\"].min()\n",
    "max_val = user_stats[\"successful_10x_predictions_ratio\"].max()\n",
    "\n",
    "if min_val == max_val:\n",
    "    user_stats[\"successful_10x_predictions_ratio_normalized\"] = 1.0  # or 0.5, depending on preference\n",
    "else:\n",
    "    user_stats[\"successful_10x_predictions_ratio_normalized\"] = (\n",
    "        (user_stats[\"successful_10x_predictions_ratio\"] - min_val) /\n",
    "        (max_val - min_val)\n",
    "    )\n",
    "\n",
    "# Avoid division by zero by checking if all values are the same\n",
    "min_val = result_df[\"longest_tweets_streak\"].min()\n",
    "max_val = result_df[\"longest_tweets_streak\"].max()\n",
    "\n",
    "if min_val == max_val:\n",
    "    result_df[\"longest_tweets_streak_normalized\"] = 1.0  # or 0.5, depending on preference\n",
    "else:\n",
    "    result_df[\"longest_tweets_streak_normalized\"] = (\n",
    "        (result_df[\"longest_tweets_streak\"] - min_val) /\n",
    "        (max_val - min_val)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicition_success = pd.DataFrame()\n",
    "\n",
    "predicition_success[\"username\"] = result_df[\"username\"]\n",
    "predicition_success[\"prediction_success_score\"] = user_stats[\"successful_10x_predictions_ratio_normalized\"] * 0.7 + result_df[\"longest_tweets_streak_normalized\"] * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicition_success[\"prediction_success_score\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False Prediction Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_buy_df[\"incorrect_buy_signal\"] = incorrect_buy_df[\"successful_predictions\"].apply(lambda x: 1 if x == 0 else 0)\n",
    "\n",
    "incorrect_buy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by userName and calculate statistics\n",
    "false_prediction_df = incorrect_buy_df.groupby('username').agg({\n",
    "    'incorrect_buy_signal': 'sum',\n",
    "    'id': 'count'  # Count of tweets per user\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate success ratio\n",
    "false_prediction_df['incorrect_buy_signal_inverse'] = 1 - (false_prediction_df['incorrect_buy_signal'] / false_prediction_df['id'])\n",
    "false_prediction_df['incorrect_buy_signal_inverse'] = false_prediction_df['incorrect_buy_signal_inverse'].fillna(0)  # Handle division by zero\n",
    "\n",
    "# Rename columns for clarity\n",
    "false_prediction_df = false_prediction_df.rename(columns={'id': 'tweet_count'})\n",
    "\n",
    "# Sort by success ratio in descending order\n",
    "false_prediction_df = false_prediction_df.sort_values('incorrect_buy_signal_inverse', ascending=False)\n",
    "\n",
    "false_prediction_df[[\"username\",\"incorrect_buy_signal_inverse\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding missed users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(false_prediction_df[\"username\"]) , len(user_stats[\"username\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find missing usernames\n",
    "missing_usernames = set(user_stats[\"username\"]) - set(false_prediction_df[\"username\"])\n",
    "\n",
    "# Create a DataFrame for missing users with default values\n",
    "missing_users_df = pd.DataFrame({\n",
    "    \"username\": list(missing_usernames),\n",
    "    \"incorrect_buy_signal\": 0,\n",
    "    \"tweet_count\": 0,\n",
    "    \"incorrect_buy_signal_inverse\": 1  # Set inverse column to 1 for all missing rows\n",
    "})\n",
    "\n",
    "# Append missing users to false_prediction_df\n",
    "false_prediction_df = pd.concat([false_prediction_df, missing_users_df], ignore_index=True)\n",
    "\n",
    "# Ensure correct column order\n",
    "false_prediction_df = false_prediction_df[[\"username\", \"incorrect_buy_signal\", \"tweet_count\", \"incorrect_buy_signal_inverse\"]]\n",
    "\n",
    "# Verify results\n",
    "false_prediction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(false_prediction_df[\"username\"]) , len(user_stats[\"username\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historical Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_score = pd.DataFrame()\n",
    "\n",
    "hist_score[\"username\"] = predicition_success[\"username\"]\n",
    "hist_score[\"prediction_success_rate\"] = predicition_success[\"prediction_success_score\"]\n",
    "hist_score[\"false_prediction_rate\"] = false_prediction_df[\"incorrect_buy_signal_inverse\"]\n",
    "\n",
    "hist_score[\"score\"] = hist_score[\"prediction_success_rate\"] * 0.7 + hist_score[\"false_prediction_rate\"] * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_score[\"score\"].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
